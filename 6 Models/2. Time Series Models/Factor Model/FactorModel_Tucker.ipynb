{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDsxFXIVrqMa",
        "outputId": "a90ae628-ee97-4901-db6b-dfd13e653295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1Yu44r-rauK",
        "outputId": "2efe2d8b-7b74-4ceb-8f32-62ba43bf3261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorly in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorly) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tensorly) (1.16.1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "!pip install tensorly\n",
        "import tensorly as tl\n",
        "from tensorly.decomposition import tucker\n",
        "from statsmodels.tsa.api import VAR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorly.tenalg import multi_mode_dot\n",
        "from tensorly import tensor as tl_tensor\n",
        "import time\n",
        "from tensorly import tucker_to_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading and check for NaNs (land data) that need to be masked"
      ],
      "metadata": {
        "id": "cuzdDUStC6gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import datasets\n",
        "train_tensor = np.load(\"/content/drive/MyDrive/LSE Capstone G Drive/Train Val Test Sets/Tensor 4D/train_tensor_new_split.npy\")\n",
        "val_tensor = np.load(\"/content/drive/MyDrive/LSE Capstone G Drive/Train Val Test Sets/Tensor 4D/val_tensor_new_split.npy\")\n",
        "test_tensor = np.load(\"/content/drive/MyDrive/LSE Capstone G Drive/Train Val Test Sets/Tensor 4D/test_tensor_new_split.npy\")\n",
        "\n",
        "print(f\"Train tensor shape: {train_tensor.shape}\")\n",
        "print(f\"Val tensor shape: {val_tensor.shape}\")\n",
        "print(f\"Test tensor shape: {test_tensor.shape}\")"
      ],
      "metadata": {
        "id": "NYYb5cJcryyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_count = np.isnan(train_tensor).sum()\n",
        "T, V, H, W = train_tensor.shape\n",
        "print(f\"Number of NaNs in train_tensor: {nan_count}\")\n",
        "print(f\"Train tensor values: {T*V*H*W}\")\n",
        "\n",
        "nan_count = np.isnan(val_tensor).sum()\n",
        "T, V, H, W = val_tensor.shape\n",
        "print(f\"Number of NaNs in val_tensor: {nan_count}\")\n",
        "print(f\"Val tensor values: {T*V*H*W}\")\n",
        "\n",
        "nan_count = np.isnan(test_tensor).sum()\n",
        "T, V, H, W = test_tensor.shape\n",
        "print(f\"Number of NaNs in test_tensor: {nan_count}\")\n",
        "print(f\"test tensor values: {T*V*H*W}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEMkF9YAtu62",
        "outputId": "5d6dbecc-3bac-4ee9-cb26-6ca935f803e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaNs in train_tensor: 11172120\n",
            "Train tensor values: 51704856\n",
            "Number of NaNs in val_tensor: 3541920\n",
            "Val tensor values: 16392096\n",
            "Number of NaNs in test_tensor: 5388240\n",
            "test tensor values: 24936912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tucker Decomposition"
      ],
      "metadata": {
        "id": "-n9VvVrWDBTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tucker decomposition function: used only on the training set"
      ],
      "metadata": {
        "id": "7qhu1w7ODExc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decompose_spatial_modes(tensor, r_lat, r_lon, r_time=None):\n",
        "    T, V, Lat, Lon = tensor.shape\n",
        "\n",
        "    # Check if val_tensor has any NaNs before proceeding\n",
        "    if np.isnan(tensor).any():\n",
        "        print(\"⚠️ Warning: NaNs detected in val_tensor. Proceeding to fill with 0.0.\")\n",
        "        tensor = np.nan_to_num(tensor, nan=0.0)\n",
        "    else:\n",
        "        tensor = tensor.copy()\n",
        "        print(\"Train has no NaNs.\")\n",
        "\n",
        "    tensor_tl = tl_tensor(tensor)\n",
        "\n",
        "    print(f\"Running Tucker on tensor with shape: {tensor.shape}\")\n",
        "    start = time.time()\n",
        "\n",
        "    # Use full rank for time if not specified\n",
        "    if r_time is None:\n",
        "        r_time = T\n",
        "\n",
        "    # Set ranks for all modes: time, vars, lat, lon\n",
        "    ranks = [r_time, V, r_lat, r_lon]\n",
        "\n",
        "    core, factors = tucker(\n",
        "        tensor_tl,\n",
        "        rank=ranks,\n",
        "        n_iter_max=100,\n",
        "        tol=1e-5\n",
        "    )\n",
        "\n",
        "    print(f\"Decomposition complete in {time.time() - start:.2f}s\")\n",
        "\n",
        "    print(\"Type of factors:\", type(factors))\n",
        "    print(\"Length of factors:\", len(factors))\n",
        "    print(\"Types of each factor:\", [type(f) for f in factors])\n",
        "    print(\"Shapes (if available):\", [getattr(f, 'shape', 'N/A') for f in factors])\n",
        "\n",
        "    U_time, U_vars, U_lat, U_lon = factors\n",
        "\n",
        "    # Convert to NumPy arrays if needed\n",
        "    U_time = np.array(U_time)\n",
        "    U_vars = np.array(U_vars)\n",
        "    U_lat = np.array(U_lat)\n",
        "    U_lon = np.array(U_lon)\n",
        "\n",
        "    print(f\"U_time shape: {U_time.shape}\")\n",
        "    print(f\"U_vars shape: {U_vars.shape}\")\n",
        "    print(f\"U_lat shape: {U_lat.shape}\")\n",
        "    print(f\"U_lon shape: {U_lon.shape}\")\n",
        "\n",
        "    core_tensor = core\n",
        "    print(\"Core tensor shape:\", core_tensor.shape)\n",
        "\n",
        "    return core_tensor, U_time, U_vars, U_lat, U_lon"
      ],
      "metadata": {
        "id": "E8YQxZfozNKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute one-step ahead VAR forecasts and evaluate the metrics"
      ],
      "metadata": {
        "id": "iqltxZ0XDJai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smape(y_true, y_pred):\n",
        "    numerator = np.abs(y_pred - y_true)\n",
        "    denominator = (np.abs(y_pred) + np.abs(y_true)) + 1e-8\n",
        "    return np.mean(numerator / denominator) * 100\n",
        "\n",
        "\n",
        "def one_step_var_forecast(core_sequence, var_model):\n",
        "    \"\"\"\n",
        "    Forecast next timestep using previous (VAR(1)) from a sequence of core vectors.\n",
        "    core_sequence: [T+1, D] including last train step + rest of val/test\n",
        "    Returns: forecasted [T, D]\n",
        "    \"\"\"\n",
        "    T, D = core_sequence.shape\n",
        "    forecasts = []\n",
        "    for t in range(1, T):\n",
        "        y_pred = var_model.forecast(core_sequence[t-1:t], steps=1)[0]\n",
        "        forecasts.append(y_pred)\n",
        "    return np.array(forecasts)\n",
        "\n",
        "\n",
        "def evaluate_split(original_tensor, reconstructed_tensor, split_name, r_time, r_lat, r_lon):\n",
        "    \"\"\"\n",
        "    Computes per-variable RMSE, MAE, SMAPE for a given data split\n",
        "    \"\"\"\n",
        "    mask = ~np.isnan(original_tensor)\n",
        "    metrics = []\n",
        "    V = original_tensor.shape[1]\n",
        "\n",
        "    for v in range(V):\n",
        "        y_true = original_tensor[:, v, :, :][mask[:, v, :, :]]\n",
        "        y_pred = reconstructed_tensor[:, v, :, :][mask[:, v, :, :]]\n",
        "\n",
        "        rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
        "        mae = np.mean(np.abs(y_pred - y_true))\n",
        "        smape_val = smape(y_true, y_pred)\n",
        "\n",
        "        metrics.append({\n",
        "            'split': split_name,\n",
        "            'variable': v,\n",
        "            'r_time': r_time,\n",
        "            'r_lat': r_lat,\n",
        "            'r_lon': r_lon,\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'SMAPE': smape_val\n",
        "        })\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "RjwlIBqdKpIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank approximation for the tensor\n",
        "$$\n",
        "X = \\sum_{r=1}^R a_r^{(1)} \\times \\cdots \\times a_r^{(N)}\n",
        "$$\n",
        "The rank of a tensor denoted by $rank(X)$ is the smallest positive integer $R$ such that the above decomposition is exact, where ”exact” means that the equality\n",
        "holds above. A decomposition of a tensor in the form where $R$ = $rank(X)$ is called the rank decomposition of $X$."
      ],
      "metadata": {
        "id": "7aeuUMLRuqTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for Tucker Decomposition\n",
        "In terms of factor analysis, the Tucker Decomposition has a multi-linear factor form, consisting of a low-dimensional tensor that is assumed to vary over time and a set of fixed factor loading matrices that do not vary over time. Therefore, the Tucker Decomposition relates the high-dimensional tensor observations to low-dimensional tensor factors.\n",
        "\n",
        "$M_t$ and $E_t$ are the corresponding signal and noise components of $X_t$, respectively. We assume that $E_t$ are uncorrelated across time. Tucker decomposition approximates the signal \\(M_t\\) as follows:\n",
        "\n",
        "$$\n",
        "    X_t = {M}_t + {E}_t = {F}_t \\times_1 U_1 \\times_2 U_2 + {E}_t\n",
        "$$\n",
        "\n",
        "${F}_t \\in \\mathbb{R}^{r_1 \\times r_2}$ is the time-dependent core tensor (compressed representation) capturing latent ocean dynamics, which we would like to model over time. $U_k$ are the {factor loading matrices}, fixed over time: $U_1 \\in \\mathbb{R}^{d_1 \\times r_1}$ is the spatial factor matrix and $U_2 \\in \\mathbb{R}^{d_2 \\times r_2}$ is the ocean charactersitics factor matrix.\n"
      ],
      "metadata": {
        "id": "akr1vJ4vf-lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_tucker_ranks(r_time_vals, lat_lon_pairs):\n",
        "\n",
        "    for r_time in r_time_vals:\n",
        "        for r_lat, r_lon in lat_lon_pairs:\n",
        "            print(f\"\\nRunning Tucker with ranks - Time: {r_time}, Lat: {r_lat}, Lon: {r_lon}\")\n",
        "\n",
        "            # Reload the train and validation sets just to be safe\n",
        "            train_tensor = np.load(\"/content/drive/MyDrive/LSE Capstone G Drive/Tensor Creation/train_tensor.npy\")\n",
        "            val_tensor = np.load(\"/content/drive/MyDrive/LSE Capstone G Drive/Tensor Creation/val_tensor.npy\")\n",
        "            test_tensor = np.load(\"/content/drive/MyDrive/LSE Capstone G Drive/Tensor Creation/test_tensor.npy\")\n",
        "\n",
        "            core_tensor, U_time, U_vars, U_lat, U_lon = decompose_spatial_modes(train_tensor, r_lat, r_lon, r_time)\n",
        "\n",
        "            # Flatten for VAR\n",
        "            F_matrix = core_tensor.reshape(core_tensor.shape[0], -1)\n",
        "            print(f\"F_matrix for VAR shape: {F_matrix.shape}\")\n",
        "\n",
        "            # Run VAR\n",
        "            model = VAR(F_matrix.T)\n",
        "            results = model.fit(maxlags=1)\n",
        "            print(results.summary())\n",
        "\n",
        "            # --- Evaluate TRAIN ---\n",
        "            reconstructed_train = tucker_to_tensor((core_tensor, [U_time, U_vars, U_lat, U_lon]))\n",
        "            if np.isnan(train_tensor).any():\n",
        "                print(\"⚠️ Warning: NaNs detected in val_tensor. Using mask to ignore them during evaluation.\")\n",
        "                mask_train = ~np.isnan(train_tensor)\n",
        "            else:\n",
        "                mask_train = np.ones_like(train_tensor, dtype=bool)\n",
        "                print(\"Train has no NaNs.\")\n",
        "\n",
        "            for v in range(train_tensor.shape[1]):\n",
        "                original_v = train_tensor[:, v, :, :]\n",
        "                recon_v = reconstructed_train[:, v, :, :]\n",
        "                mask_v = mask_train[:, v, :, :]\n",
        "\n",
        "                y_true = original_v[mask_v]\n",
        "                y_pred = recon_v[mask_v]\n",
        "\n",
        "                mse_v = np.mean((y_pred - y_true) ** 2)\n",
        "                rmse_v = np.sqrt(mse_v)\n",
        "                mae_v = np.mean(np.abs(y_pred - y_true))\n",
        "                smape_v = smape(y_true, y_pred)\n",
        "\n",
        "                records.append({\n",
        "                    'split': 'train',\n",
        "                    'variable': v,\n",
        "                    'r_time': r_time,\n",
        "                    'r_lat': r_lat,\n",
        "                    'r_lon': r_lon,\n",
        "                    'RMSE': rmse_v,\n",
        "                    'MAE': mae_v,\n",
        "                    'SMAPE': smape_v\n",
        "                })\n",
        "\n",
        "            # --- Evaluate VALIDATION ---\n",
        "            # Check if val_tensor has any NaNs before proceeding\n",
        "            if np.isnan(val_tensor).any():\n",
        "                print(\"⚠️ Warning: NaNs detected in val_tensor. Proceeding to fill with 0.0.\")\n",
        "                val_tensor_filled = np.nan_to_num(val_tensor, nan=0.0)\n",
        "                used_nan_filling = True\n",
        "            else:\n",
        "                val_tensor_filled = val_tensor.copy()\n",
        "                used_nan_filling = False\n",
        "\n",
        "            # Project to core space\n",
        "            core_val = multi_mode_dot(\n",
        "                tl.tensor(val_tensor_filled),\n",
        "                [U_vars.T, U_lat.T, U_lon.T],\n",
        "                modes=[1, 2, 3]\n",
        "            )\n",
        "\n",
        "            # Reconstruct\n",
        "            reconstructed_val = multi_mode_dot(\n",
        "                core_val,\n",
        "                [U_vars, U_lat, U_lon],\n",
        "                modes=[1, 2, 3]\n",
        "            )\n",
        "\n",
        "            # Mask for evaluation\n",
        "            if np.isnan(val_tensor).any():\n",
        "                print(\"⚠️ Warning: NaNs detected in val_tensor. Using mask to ignore them during evaluation.\")\n",
        "                mask_val = ~np.isnan(val_tensor)\n",
        "                used_masking = True\n",
        "            else:\n",
        "                mask_val = np.ones_like(val_tensor, dtype=bool)\n",
        "                used_masking = False\n",
        "\n",
        "            # Optional: summary print\n",
        "            if not used_nan_filling and not used_masking:\n",
        "                print(\"✅ No NaNs detected in val_tensor. No filling or masking applied.\")\n",
        "\n",
        "            for v in range(val_tensor.shape[1]):\n",
        "                original_v = val_tensor[:, v, :, :]\n",
        "                recon_v = reconstructed_val[:, v, :, :]\n",
        "                mask_v = mask_val[:, v, :, :]\n",
        "\n",
        "                y_true = original_v[mask_v]\n",
        "                y_pred = recon_v[mask_v]\n",
        "\n",
        "                mse_v = np.mean((y_pred - y_true) ** 2)\n",
        "                rmse_v = np.sqrt(mse_v)\n",
        "                mae_v = np.mean(np.abs(y_pred - y_true))\n",
        "                smape_v = smape(y_true, y_pred)\n",
        "\n",
        "                records.append({\n",
        "                    'split': 'validation',\n",
        "                    'variable': v,\n",
        "                    'r_time': r_time,\n",
        "                    'r_lat': r_lat,\n",
        "                    'r_lon': r_lon,\n",
        "                    'RMSE': rmse_v,\n",
        "                    'MAE': mae_v,\n",
        "                    'SMAPE': smape_v\n",
        "                })\n",
        "\n",
        "    # Save to DataFrame and CSV\n",
        "    df_results = pd.DataFrame(records)\n",
        "    df_results.to_csv(\"tucker_eval_per_variable.csv\", index=False)\n",
        "    print(\"Saved all results to 'tucker_eval_per_variable.csv'\")"
      ],
      "metadata": {
        "id": "b5jZMYX_Bs8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating this model for various ranks\n",
        "We show that for certain pairs, this model fails to work; such as when time_rank = 100, variable_rank = 8, lat_rank = 2, lon_rank = 2. While the tucker decomposition works, the VAR model tries to fit a matrix with 100 time points and 32 features -> (100,32) and fails because there are too few observations to estimate the 32x32 coefficient matrix."
      ],
      "metadata": {
        "id": "lkIVbqHaDWLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# r_time_vals = [20, 50, 100, 150]\n",
        "# lat_lon_pairs = [(5, 10), (15, 20), (25, 30), (35, 40), (50, 60), (60, 80), (60, 100), (60, 150)]\n",
        "r_time_vals = [400]\n",
        "lat_lon_pairs = [(2, 2)]\n",
        "\n",
        "# Run the rank approximation\n",
        "records = []\n",
        "evaluate_tucker_ranks(r_time_vals, lat_lon_pairs)\n",
        "\n",
        "# Save to CSV\n",
        "df_metrics = pd.DataFrame(records)\n",
        "df_metrics.to_csv(\"tucker_metrics_results_safe.csv\", index=False)\n",
        "print(\"Saved detailed results to tucker_metrics_results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "vrIsySRN4WIb",
        "outputId": "cbfec868-e145-45b4-ca7c-a07ef0621a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Tucker with ranks - Time: 100, Lat: 2, Lon: 2\n",
            "Train has no NaNs.\n",
            "Running Tucker on tensor with shape: (593, 8, 63, 173)\n",
            "Decomposition complete in 117.90s\n",
            "Type of factors: <class 'list'>\n",
            "Length of factors: 4\n",
            "Types of each factor: [<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'numpy.ndarray'>]\n",
            "Shapes (if available): [(593, 100), (8, 8), (63, 2), (173, 2)]\n",
            "U_time shape: (593, 100)\n",
            "U_vars shape: (8, 8)\n",
            "U_lat shape: (63, 2)\n",
            "U_lon shape: (173, 2)\n",
            "Core tensor shape: (100, 8, 2, 2)\n",
            "F_matrix for VAR shape: (100, 32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "33-th leading minor of the array is not positive definite",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2537217022.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Run the rank approximation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mevaluate_tucker_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_time_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat_lon_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Save to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3183834659.py\u001b[0m in \u001b[0;36mevaluate_tucker_ranks\u001b[0;34m(r_time_vals, lat_lon_pairs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# --- Evaluate TRAIN ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/vector_ar/var_model.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mVARSummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVARSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mirf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_decomp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/vector_ar/output.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/vector_ar/output.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, endog_names, exog_names)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_header_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stats_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coef_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resid_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/vector_ar/output.py\u001b[0m in \u001b[0;36m_stats_table\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m                        \u001b[0;34m'FPE:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                        'Det(Omega_mle):')\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mpart2Ldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mpart2Rdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhqic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetomega\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mpart2Lheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mproperties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/vector_ar/var_model.py\u001b[0m in \u001b[0;36mllf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mllf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;34m\"Compute VAR(p) loglikelihood\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvar_loglike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_u_mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/vector_ar/var_model.py\u001b[0m in \u001b[0;36mvar_loglike\u001b[0;34m(resid, omega, nobs)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m\\\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mln\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mOmega\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mln\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mlogdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogdet_symm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0mneqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mpart1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnobs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mneqs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tools/linalg.py\u001b[0m in \u001b[0;36mlogdet_symm\u001b[0;34m(m, check_symm)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# would be nice to short-circuit check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m is not symmetric.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcho_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0;31m# Early exit if call is not batched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mother_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;31m# Determine broadcasted batch shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36mcho_factor\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \"\"\"\n\u001b[0;32m--> 183\u001b[0;31m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=False,\n\u001b[0m\u001b[1;32m    184\u001b[0m                          check_finite=check_finite)\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpotrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         raise LinAlgError(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;34mf\"{info}-th leading minor of the array is not positive definite\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         )\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 33-th leading minor of the array is not positive definite"
          ]
        }
      ]
    }
  ]
}